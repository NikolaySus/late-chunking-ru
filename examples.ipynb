{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1173893c4f0ea56",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Chunked Pooling\n",
    "This notebooks explains how the chunked pooling can be implemented. First you need to install the requirements: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373374ba-f326-49db-98db-d2196488054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a920f-cde0-4035-9834-49b087aab5cc",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8fbc1e477db48",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Then we load a model which we want to use for the embedding. We choose `jinaai/jina-embeddings-v2-base-en` but any other model which supports mean pooling is possible. However, models with a large maximum context-length are preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa028ea-d592-44c1-9e9d-b6bccd470316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunked_pooling import chunked_pooling, chunk_by_sentences\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380abf7acde9517",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v3', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v3', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0c1162797ffb0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we define the text which we want to encode and split it into chunks. The `chunk_by_sentences` function also returns the span annotations. Those specify the number of tokens per chunk which is needed for the chunked pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef392f3437ef82e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"Москва — столица России, город федерального значения, административный центр Центрального федерального округа и центр Московской области, в состав которой не входит. Мегаполис; крупнейший по численности населения город России и её субъект — 13 149 803 человека (2024), что делает Москву 22-й среди городов мира по численности населения. Центр Московской городской агломерации. Самый крупный город Европы по площади и населению.\"\n",
    "#input_text = \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
    "\n",
    "# determine chunks\n",
    "chunks, span_annotations = chunk_by_sentences(input_text, tokenizer)\n",
    "print('Chunks:\\n- \"' + '\"\\n- \"'.join(chunks) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac41fd1f0560da7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we encode the chunks with the traditional and the context-sensitive chunked pooling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe3d93b9e6609b9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# chunk before\n",
    "embeddings_traditional_chunking = model.encode(chunks)\n",
    "\n",
    "# chunk afterwards (context-sensitive chunked pooling)\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "model_output = model(**inputs)\n",
    "embeddings = chunked_pooling(model_output, [span_annotations])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b1b9d48cb6367",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Finally, we compare the similarity of the word \"Berlin\" with the chunks. The similarity should be higher for the context-sensitive chunked pooling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cec59a3ece76",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "test_text = 'Москва'#\"Berlin\"#\n",
    "berlin_embedding = model.encode(test_text)\n",
    "\n",
    "for chunk, new_embedding, trad_embeddings in zip(chunks, embeddings, embeddings_traditional_chunking):\n",
    "    print(f'similarity_new(\"{test_text}\", \"{chunk}\"):', cos_sim(berlin_embedding, new_embedding))\n",
    "    print(f'similarity_trad(\"{test_text}\", \"{chunk}\"):', cos_sim(berlin_embedding, trad_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f8fce-4909-4fb9-b6ec-0321aac53f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk, new_embedding, trad_embeddings in zip(chunks, embeddings, embeddings_traditional_chunking):\n",
    "    print(f'abs new(\"{chunk}\"):', np.linalg.norm(new_embedding))\n",
    "    print(f'abs trad(\"{chunk}\"):', np.linalg.norm(trad_embeddings))\n",
    "\n",
    "print(f'\\nabs test_text(\"{test_text}\"):', np.linalg.norm(berlin_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdd36c-74d6-4d09-8c13-73f97b88d6bc",
   "metadata": {},
   "source": [
    "# Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cd1695-b616-4e94-9fa1-e8f080418dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunked_pooling import chunked_pooling, chunk_by_sentences\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4dd33e-fa2c-49d7-9831-dd1ae37e6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = os.path.abspath('')\n",
    "queries = pd.read_json(basePath + \"\\\\ai-forever-ria-news-retrieval\\\\queries.jsonl\", lines=True)\n",
    "corpus = pd.read_json(basePath + \"\\\\ai-forever-ria-news-retrieval\\\\corpus.jsonl\", lines=True)\n",
    "test = pd.read_json(basePath + \"\\\\ai-forever-ria-news-retrieval\\\\test.jsonl\", lines=True)\n",
    "del corpus['title']\n",
    "del test['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c69626-9f6a-459f-b652-f3ae928ec79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>премьер-министр украины, кандидат в президенты...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>группа вооруженных людей в ночь с субботы на ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>немецкий теннисист михаэль беррер стал победи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>генеральный секретарь оон пан ги мун заявил в...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>леверкузенский \"байер\" со счетом 3:1 на свое...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704339</th>\n",
       "      <td>704339</td>\n",
       "      <td>главными стратегическими учениями для армии ро...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704340</th>\n",
       "      <td>704340</td>\n",
       "      <td>ракетные войска стратегического назначения (р...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704341</th>\n",
       "      <td>704341</td>\n",
       "      <td>сухопутные войска россии в 2015 году примут у...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704342</th>\n",
       "      <td>704342</td>\n",
       "      <td>полиция мексиканского города чилапа в штате ге...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704343</th>\n",
       "      <td>704343</td>\n",
       "      <td>крупный пожар начался в одном из цехов в севе...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704344 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           _id                                               text\n",
       "0            0  премьер-министр украины, кандидат в президенты...\n",
       "1            1  группа вооруженных людей в ночь с субботы на ...\n",
       "2            2  немецкий теннисист михаэль беррер стал победи...\n",
       "3            3  генеральный секретарь оон пан ги мун заявил в...\n",
       "4            4  леверкузенский \"байер\" со счетом 3:1 на свое...\n",
       "...        ...                                                ...\n",
       "704339  704339  главными стратегическими учениями для армии ро...\n",
       "704340  704340  ракетные войска стратегического назначения (р...\n",
       "704341  704341  сухопутные войска россии в 2015 году примут у...\n",
       "704342  704342  полиция мексиканского города чилапа в штате ге...\n",
       "704343  704343  крупный пожар начался в одном из цехов в севе...\n",
       "\n",
       "[704344 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6351242-3002-4771-943d-ad1e5643c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ch_emb(calc_chunk):\n",
    "    device = torch.device(\"cuda\")\n",
    "    # load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v3', trust_remote_code=True, device_map = 'cuda')\n",
    "    model = AutoModel.from_pretrained('jinaai/jina-embeddings-v3', trust_remote_code=True, device_map = 'cuda')\n",
    "    result = pd.DataFrame({\n",
    "        \"chunk\" : [],\n",
    "        \"trad_chunk_embedding\" : [],\n",
    "        \"new_chunk_embedding\" : [],\n",
    "        \"doc_id\" : [],\n",
    "    })\n",
    "    result.astype('object')\n",
    "    for doc_id, doc in calc_chunk.iterrows():\n",
    "        doc_chunks, doc_span_annotations = chunk_by_sentences(doc['text'], tokenizer)\n",
    "        doc_trad_chunk_embeddings = model.encode(doc_chunks)\n",
    "        doc_inputs = tokenizer(doc['text'], return_tensors='pt')\n",
    "        doc_model_output = model(**(doc_inputs.to(device)))\n",
    "        doc_new_chunk_embeddings = chunked_pooling(doc_model_output, [doc_span_annotations])[0]\n",
    "        for i, (chunk, trad_chunk_embedding, new_chunk_embedding) in enumerate(zip(doc_chunks, doc_trad_chunk_embeddings, doc_new_chunk_embeddings)):\n",
    "            result.loc[i] = np.array([chunk, trad_chunk_embedding, new_chunk_embedding, doc['_id']], dtype=object)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97decb0d-9792-4c62-83ca-b3f333902a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from threading import Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4adf6448-2bad-4538-9bb8-efe2ffa393d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_processes = cpu_count()\n",
    "num_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcf53da5-75e4-425e-8b9d-1132a087d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1367598c-3a7d-49a1-8140-c22397172630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704344"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92603e8d-caa7-40d8-ae5e-9f69e429a2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352172"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_chunk_size = int(corpus.shape[0]/num_processes)\n",
    "calc_chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bdac29e-cbe1-49ec-848a-14eaca8c2d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352172"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_calc_chunk_size = corpus.shape[0] - num_processes * calc_chunk_size + calc_chunk_size\n",
    "last_calc_chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b115efdd-bdcd-40b2-a7e1-0a779ff3b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the chunks\n",
    "calc_chunks = []\n",
    "\n",
    "# Create the regular chunks\n",
    "for i in range(num_processes - 1):\n",
    "    start_idx = i * calc_chunk_size\n",
    "    end_idx = start_idx + calc_chunk_size\n",
    "    calc_chunks.append(corpus.iloc[start_idx:end_idx])\n",
    "\n",
    "# Handle the last chunk separately\n",
    "start_idx = (num_processes - 1) * calc_chunk_size\n",
    "end_idx = start_idx + last_calc_chunk_size\n",
    "calc_chunks.append(corpus.iloc[start_idx:end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "333c4399-686e-46e8-8dad-eafeee3a802d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    }
   ],
   "source": [
    "# create our pool with `num_processes` processes\n",
    "pool = Pool(processes=num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c503ab-e45c-4bf4-8a08-4537e48e85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# apply our function to each chunk in the list\n",
    "result = pool.map(do_ch_emb, calc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a877a91-b5c8-49fa-a4ff-fbb2b927c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.concat(result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a7be7-2e03-47fb-be0a-0ad5d39cba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks.to_pickle(basePath + \"\\\\ai-forever-ria-news-retrieval\\\\chunks_embedded.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40956bd-f81d-4a54-ad21-25383b498e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "queries['embedding'] = queries.apply(lambda q: model.encode(q['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe2fa8-321c-4884-b0f5-9b1b42f56ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac98d0a-e423-4dd1-ae5f-47bba7f0ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries.to_pickle(basePath + \"\\\\ai-forever-ria-news-retrieval\\\\queries_embedded.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c7c22-4a7e-4d95-ab13-fc335ae9b39e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
